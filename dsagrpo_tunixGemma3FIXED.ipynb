{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 119261,
          "databundleVersionId": 14363498,
          "sourceType": "competition"
        },
        {
          "sourceId": 7045423,
          "sourceType": "datasetVersion",
          "datasetId": 4054119,
          "isSourceIdPinned": false
        },
        {
          "sourceId": 277797,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 237774,
          "modelId": 222398
        }
      ],
      "dockerImageVersionId": 31194,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "gpuType": "V5E1",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00bee22dbb1d47c59dc78bf365ab7a20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_878f1ebcd8144381bf64cd2a978d5048",
              "IPY_MODEL_411b782955f64dd78131c39bad1a71e8",
              "IPY_MODEL_0dadc11700fc4fba97d18f64b7b92ac0",
              "IPY_MODEL_ef7219b16cd9417a89e1081f5957a77e",
              "IPY_MODEL_31f9604751994541bc2140ae721c9179"
            ],
            "layout": "IPY_MODEL_3af59359013346a497ad5df684e879d6"
          }
        },
        "878f1ebcd8144381bf64cd2a978d5048": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c7d606d5be84a778611d4704f601b81",
            "placeholder": "​",
            "style": "IPY_MODEL_20a3f50fd29d4e63add7f69ba7660ef8",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "411b782955f64dd78131c39bad1a71e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_b62735f7708741088dbe63713d23ef9f",
            "placeholder": "​",
            "style": "IPY_MODEL_4e425844e81848209dc88d81b05f6426",
            "value": ""
          }
        },
        "0dadc11700fc4fba97d18f64b7b92ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_687240cba40e405484b985bdd3bb6759",
            "placeholder": "​",
            "style": "IPY_MODEL_0deb37b87c1747b8b02e33494ad3cbe3",
            "value": ""
          }
        },
        "ef7219b16cd9417a89e1081f5957a77e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_4751ec817941488a94a93bb57387c9dd",
            "style": "IPY_MODEL_5f12640af0624ef7ad1b1848562e0471",
            "tooltip": ""
          }
        },
        "31f9604751994541bc2140ae721c9179": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39ee94b658ce4ebcbf9fbe9b679501f0",
            "placeholder": "​",
            "style": "IPY_MODEL_10df9f18d7e1438dbc59b24b0a74b3aa",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "3af59359013346a497ad5df684e879d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "8c7d606d5be84a778611d4704f601b81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20a3f50fd29d4e63add7f69ba7660ef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b62735f7708741088dbe63713d23ef9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e425844e81848209dc88d81b05f6426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "687240cba40e405484b985bdd3bb6759": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0deb37b87c1747b8b02e33494ad3cbe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4751ec817941488a94a93bb57387c9dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f12640af0624ef7ad1b1848562e0471": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "39ee94b658ce4ebcbf9fbe9b679501f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10df9f18d7e1438dbc59b24b0a74b3aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fec9f48206b44c29c128c9fc056aa1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_412abc012fb142629950545491fd932d",
              "IPY_MODEL_bffe0583f77341e48edff337721e9fc0",
              "IPY_MODEL_30d5c35f19a0489fbc1cb7e22eb954eb"
            ],
            "layout": "IPY_MODEL_d7feac8adf7f4072b142423c7f74ab20"
          }
        },
        "412abc012fb142629950545491fd932d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ae2408952e54ed9b8b2ae8130947237",
            "placeholder": "​",
            "style": "IPY_MODEL_21574922368e44f69dcab873e5d5f7ee",
            "value": "100%"
          }
        },
        "bffe0583f77341e48edff337721e9fc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22442cc4bdc44225961bf8c70c03a4a0",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d39df45616c420381a6960a69ed4afb",
            "value": 100
          }
        },
        "30d5c35f19a0489fbc1cb7e22eb954eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_312703de07a54b599c3608c0b5c2c941",
            "placeholder": "​",
            "style": "IPY_MODEL_9f46c857cb224a2c9e40714901f99e53",
            "value": " 100/100 [01:29&lt;00:00,  2.29it/s]"
          }
        },
        "d7feac8adf7f4072b142423c7f74ab20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ae2408952e54ed9b8b2ae8130947237": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21574922368e44f69dcab873e5d5f7ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22442cc4bdc44225961bf8c70c03a4a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d39df45616c420381a6960a69ed4afb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "312703de07a54b599c3608c0b5c2c941": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f46c857cb224a2c9e40714901f99e53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b092aa266c854d85b64c492cd14be50c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9272ecb1985241cabf1d2519e9e10d80",
              "IPY_MODEL_f78bc9682e28481da8430ea756627b62",
              "IPY_MODEL_d7427e66b0ff4b98957556d47a66d900"
            ],
            "layout": "IPY_MODEL_dc0d01f1926e454ca0ca6862d72833bc"
          }
        },
        "9272ecb1985241cabf1d2519e9e10d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8dc2d7b5ab0478f9a9c4e94ce688f32",
            "placeholder": "​",
            "style": "IPY_MODEL_238d92c999214a5b90bb2c49e40d2660",
            "value": "Actor Training:   0%"
          }
        },
        "f78bc9682e28481da8430ea756627b62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08a67ed427644398b483acd420c1ba6d",
            "max": 3738,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35332e2e1ec44419894355c80307ac45",
            "value": 0
          }
        },
        "d7427e66b0ff4b98957556d47a66d900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a8b22ec22b04d2caa746b782ea6429d",
            "placeholder": "​",
            "style": "IPY_MODEL_6138751932604e27a97f0b8887022ed2",
            "value": " 0/3738 [00:00&lt;?, ?step/s]"
          }
        },
        "dc0d01f1926e454ca0ca6862d72833bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "b8dc2d7b5ab0478f9a9c4e94ce688f32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "238d92c999214a5b90bb2c49e40d2660": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08a67ed427644398b483acd420c1ba6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35332e2e1ec44419894355c80307ac45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a8b22ec22b04d2caa746b782ea6429d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6138751932604e27a97f0b8887022ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwycoff2013/dual-stream/blob/main/dsagrpo_tunixGemma3FIXED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dual-Stream Gemma with DSA-GRPO (Dual-Stream Aware GRPO)\n",
        "\n",
        "This notebook fine-tunes **Gemma 3 1B** on a single Kaggle TPU using **Tunix**, but instead of plain GRPO it uses an **extension I call DSA-GRPO (Dual-Stream Aware GRPO)**.\n",
        "\n",
        "The goal is not just to get good answers, but to train a model that:\n",
        "\n",
        "- **Thinks in a structured, machine-readable way** (the *Monologue Stream*), and  \n",
        "- **Stays coherent with its own thinking** when it produces the final answer (the *Answer Stream*).\n",
        "\n",
        "All model outputs follow the competition format:\n",
        "\n",
        "```text\n",
        "<reasoning>\n",
        "[plan] ...\n",
        "[evidence] ...\n",
        "[sanity_check] ...\n",
        "[risk_scan] ...\n",
        "[final_internal_action] ...\n",
        "</reasoning>\n",
        "<answer> final_answer_here </answer>\n",
        "````\n",
        "\n",
        "On the competition side, this maps directly to:\n",
        "\n",
        "* `model_thinking_trace`  ← the contents of `<reasoning>...</reasoning>`\n",
        "* `model_answer`          ← the contents of `<answer>...</answer>`\n",
        "\n",
        "---\n",
        "\n",
        "## What is DSA-GRPO?\n",
        "\n",
        "Standard **GRPO** optimizes a policy using a group of sampled completions per prompt and a set of reward functions (format, correctness, etc.) that are usually combined in a fairly simple way.\n",
        "\n",
        "**DSA-GRPO** keeps the same *infrastructure* (Tunix’s `GRPOLearner`, TPUs, Gemma3 + LoRA), but changes the **reward shaping and aggregation** so that it explicitly understands the **Dual-Stream Architecture (DSA)**:\n",
        "\n",
        "* **Monologue Stream**: everything inside `<reasoning>...</reasoning>`.\n",
        "* **Answer Stream**: the final answer inside `<answer>...</answer>`.\n",
        "\n",
        "Instead of treating each reward independently, DSA-GRPO uses a **composite reward function** `dsa_grpo_reward` that aggregates several components:\n",
        "\n",
        "* **Format rewards**\n",
        "\n",
        "  * `match_format_exactly` – strict tag/layout compliance.\n",
        "  * `match_format_approximately` – softer structural check.\n",
        "\n",
        "* **Answer correctness rewards**\n",
        "\n",
        "  * `check_answer` – string-level match against the target solution.\n",
        "  * `check_numbers` – numeric correctness / sanity.\n",
        "\n",
        "* **Dual-Stream / DSA rewards**\n",
        "\n",
        "  * `dsa_monologue_structure` – rewards structured internal monologues with sections like `[plan]`, `[evidence]`, `[sanity_check]`, `[risk_scan]`, `[final_internal_action]`, plus multi-step reasoning.\n",
        "  * `dsa_stream_coherence` – rewards agreement between the monologue’s final numeric conclusion and the `<answer>` value, and penalizes contradictions.\n",
        "\n",
        "These signals are then combined inside **one DSA-aware scalar reward**:\n",
        "\n",
        "* Each component is weighted (e.g. correctness gets more weight than formatting, coherence has its own weight, etc.).\n",
        "* If the **format is broken** (no proper `<reasoning>` / `<answer>`), the total reward is heavily down-weighted.\n",
        "* If **coherence is negative** (monologue and answer disagree), the combined reward is penalized and shrunk — discouraging “pretty explanations” that don’t match what the model actually answers.\n",
        "\n",
        "In other words, DSA-GRPO doesn’t just say “do many good things at once”; it encodes **how those things should relate** in a Dual-Stream setting:\n",
        "\n",
        "> A response is only truly good if the *format is valid*, the *answer is correct*, the *monologue is structured*, **and** the *monologue and answer agree*.\n",
        "\n",
        "---\n",
        "\n",
        "## What this notebook actually does\n",
        "\n",
        "* **Model & framework**\n",
        "\n",
        "  * Loads **Gemma 3 1B (Flax)** via **Tunix** on a Kaggle TPU.\n",
        "  * Attaches a **LoRA policy head** (via QWIX2) so we can fine-tune with RL while keeping the base weights mostly frozen.\n",
        "\n",
        "* **Dual-Stream prompting**\n",
        "\n",
        "  * Uses a DSA-aware `SYSTEM_PROMPT` that instructs the model to separate:\n",
        "\n",
        "    * `<reasoning>...</reasoning>` (Monologue Stream) and\n",
        "    * `<answer>...</answer>` (Answer Stream),\n",
        "      with explicit structure in the monologue.\n",
        "\n",
        "* **DSA-GRPO training loop**\n",
        "\n",
        "  * Instantiates an `RLCluster` with:\n",
        "\n",
        "    * `actor`   = LoRA-adapted Gemma3,\n",
        "    * `reference` = frozen Gemma3,\n",
        "    * `tokenizer` = Gemma tokenizer,\n",
        "    * `cluster_config` tuned for a single Kaggle TPU session.\n",
        "  * Wraps this in `GRPOLearner`, but sets:\n",
        "\n",
        "    ```python\n",
        "    reward_fns = [dsa_grpo_reward]\n",
        "    ```\n",
        "\n",
        "    so **all** learning is driven by the DSA-aware composite reward.\n",
        "  * Trains on a reasoning dataset (e.g. GSM8K or similar) within a 9-hour Kaggle TPU budget.\n",
        "\n",
        "* **Generation & submission**\n",
        "\n",
        "  * Provides:\n",
        "\n",
        "    * `parse_dual_stream(completion)` → `(monologue, answer)`\n",
        "    * `generate_dual_stream(question, sampler, ...)` → `(monologue, answer)`\n",
        "  * These are wired to produce the exact competition output format:\n",
        "\n",
        "    ```text\n",
        "    <reasoning>model_thinking_trace</reasoning>\n",
        "    <answer>model_answer</answer>\n",
        "    ```\n",
        "  * A small demo cell shows how to generate and print:\n",
        "\n",
        "    * `model_thinking_trace`\n",
        "    * `model_answer`\n",
        "      for a sample question.\n",
        "\n",
        "* **Checkpoints & reproducibility**\n",
        "\n",
        "  * Uses Tunix’s built-in checkpointing, so the **fine-tuned LoRA checkpoint**:\n",
        "\n",
        "    * Is produced inside the same notebook / single TPU session.\n",
        "    * Can be reloaded via the **Gemma2/3 modeling code in Tunix** on Kaggle.\n",
        "\n",
        "---\n",
        "\n",
        "## Why this is different from plain GRPO\n",
        "\n",
        "Traditional GRPO on this competition might:\n",
        "\n",
        "* Reward format,\n",
        "* Reward correctness,\n",
        "* Maybe reward “length” or “presence of chain-of-thought”.\n",
        "\n",
        "But it typically doesn’t care whether the model’s *inner story* and *outer answer* actually line up.\n",
        "\n",
        "**DSA-GRPO adds two critical properties:**\n",
        "\n",
        "1. **Monologue structure as a first-class objective**\n",
        "   The model is explicitly rewarded for producing monologues that look like real internal deliberation rather than a single line of algebra.\n",
        "\n",
        "2. **Coherence as a first-class objective**\n",
        "   The model is punished when its Monologue Stream and Answer Stream disagree. This pushes it away from post-hoc rationalizations and towards **actual alignment between thought and answer**.\n",
        "\n",
        "That makes this notebook not just “Gemma with GRPO that shows its work”, but a **Dual-Stream Gemma** that is trained to keep its own internal monologue and final answer in sync, which is exactly what the competition is trying to probe: models that don’t just talk about their reasoning, but **live inside a training signal that cares about that reasoning being coherent.**\n",
        "\n",
        "\n",
        "*all content based on the whitepaper \"The Inner Monologue: A Dual-Stream Architecture for Verifiable Inner Alignment\"- daniel wycoff*****\n",
        "\n",
        "https://docs.google.com/document/d/1np-I9zEKArodlDhQzfydhloCXIVK9O72g3OJSuo_-Wk/edit?usp=sharing\n",
        "\n",
        "*contact: daniel[dot]w[at]eorumyoung[dot]com*"
      ],
      "metadata": {
        "id": "LkpVFTOiwCAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# W&B compatibility patch for Tunix on Kaggle.\n",
        "# Some Tunix versions expect wandb.util._has_internet, which is missing\n",
        "# in newer wandb releases. This patch adds a no-op implementation so\n",
        "# wandb.init() does not crash inside Tunix metrics_logger.\n",
        "#!pip install wandb\n",
        "#try:\n",
        "#  import wandb\n",
        " # if not hasattr(wandb.util, \"_has_internet\"):\n",
        " #   def _has_internet():\n",
        "      # Treat environment as offline; prevents crashes in wandb login.\n",
        " #     return False\n",
        " #   wandb.util._has_internet = _has_internet\n",
        " #   print(\"Patched wandb.util._has_internet for Tunix.\")\n",
        "#except Exception as e:\n",
        "# print(\"W&B compatibility patch failed:\", e)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "nFJJl3PqwCAH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install necessary libraries"
      ],
      "metadata": {
        "id": "afofSj37qYz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kagglehub\n",
        "\n",
        "!pip install -q ipywidgets\n",
        "\n",
        "!pip install -q tensorflow\n",
        "!pip install -q tensorflow_datasets\n",
        "!pip install -q tensorboardX\n",
        "!pip install -q transformers\n",
        "!pip install -q grain\n",
        "!pip install \"google-tunix[prod]==0.1.3\"\n",
        "\n",
        "# !pip install -q git+https://github.com/google/tunix\n",
        "# !pip install -q git+https://github.com/google/qwix\n",
        "\n",
        "# !pip uninstall -q -y flax  # disabled: Tunix pins flax/nnx\n",
        "# !pip install -q git+https://github.com/google/flax.git\n",
        "# !pip install -U flax  # disabled: Tunix pins flax/nnx\n",
        "\n",
        "\n",
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "Z03GnyApTn1j",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "111c930f-dc13-4eae-a510-4fb063990705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-tunix==0.1.3 in /usr/local/lib/python3.12/dist-packages (from google-tunix[prod]==0.1.3) (0.1.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.4.1)\n",
            "Requirement already satisfied: flax>=0.11.1 in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.12.1)\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.10.0)\n",
            "Requirement already satisfied: grain in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.2.14)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.36.0)\n",
            "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.1.6)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.3.13)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.62.1)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.3.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.2.1)\n",
            "Requirement already satisfied: qwix in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.2.1)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.6.4)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.9.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.57.1)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.9)\n",
            "Requirement already satisfied: jax!=0.7.2,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.0.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.1.2)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.2.6)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.11.28)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.79)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (14.2.0)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.15.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.0.3)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.10)\n",
            "Requirement already satisfied: jaxlib<=0.8.1,>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (0.8.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (0.5.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (1.16.3)\n",
            "Requirement already satisfied: libtpu==0.0.30.* in /usr/local/lib/python3.12/dist-packages (from jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (0.0.30)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.2.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.70.18)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (25.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.2.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.13.2)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.12/dist-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (5.2.1)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.12/dist-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.12/dist-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.2.3)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.12/dist-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.19.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.4.0)\n",
            "Requirement already satisfied: array-record>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.8.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.1.2)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/dist-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.13.0)\n",
            "Requirement already satisfied: protobuf>=5.28.3 in /usr/local/lib/python3.12/dist-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.33.1)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from jaxtyping->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.0.3)\n",
            "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.45.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.9.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.9)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.2.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (5.9.5)\n",
            "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.17.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.2.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.10.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.7.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.22.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.8.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.23.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.2->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.2->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.2->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (2.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.19.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.28.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.5.0)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.8.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.7.1)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.90)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.6.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (25.1.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.14.0)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.20.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from promise->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.17.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.12/dist-packages (from simple_parsing->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.12/dist-packages (from tensorflow-metadata->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.72.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (75.2.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.1.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.26.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.3.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.3.1)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "LnF9ZACiTn1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import gc\n",
        "import os\n",
        "from pprint import pprint\n",
        "import re\n",
        "\n",
        "import csv\n",
        "import shutil\n",
        "\n",
        "from flax import nnx\n",
        "import grain\n",
        "import humanize\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import kagglehub\n",
        "from orbax import checkpoint as ocp\n",
        "import optax\n",
        "from pathlib import Path\n",
        "import qwix\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm.auto import tqdm\n",
        "from tunix.generate import sampler as sampler_lib\n",
        "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
        "# from tunix.models.gemma3 import model as gemma_lib\n",
        "# from tunix.models.gemma3 import params as params_lib\n",
        "from tunix.models.gemma3 import params\n",
        "from tunix.models.gemma3 import model\n",
        "from tunix.rl import rl_cluster as rl_cluster_lib\n",
        "from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n",
        "from tunix.rl.rollout import base_rollout\n",
        "from tunix.sft import metrics_logger\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "McTNo_r8Tn1k",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fc9b9cd-fe4e-41ee-aac9-4416ef2febda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Compatibility patch: make flax.nnx.Variable.set_metadata accept\n",
        "# the legacy (name, value) calling pattern used by QWIX LoRA provider.\n",
        "from flax import nnx\n",
        "\n",
        "if getattr(getattr(nnx, \"Variable\", None), \"_dsagrpo_patched\", False) is False:\n",
        "  _orig_set_metadata = nnx.Variable.set_metadata\n",
        "\n",
        "  def _dsagrpo_set_metadata(self, *args, **kwargs):\n",
        "    # Legacy patterns:\n",
        "    #   set_metadata({'sharding_names': axes})\n",
        "    #   set_metadata('sharding_names', axes)\n",
        "    if args and not kwargs:\n",
        "      if len(args) == 1 and isinstance(args[0], dict):\n",
        "        return _orig_set_metadata(self, **args[0])\n",
        "      if len(args) == 2 and isinstance(args[0], str):\n",
        "        return _orig_set_metadata(self, **{args[0]: args[1]})\n",
        "    # Modern usage: direct kwargs (e.g. set_metadata(sharding_names=axes))\n",
        "    return _orig_set_metadata(self, **kwargs)\n",
        "\n",
        "  nnx.Variable.set_metadata = _dsagrpo_set_metadata\n",
        "  nnx.Variable._dsagrpo_patched = True\n",
        "  print(\"Patched nnx.Variable.set_metadata for QWIX compatibility.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "YZGjVuVJwCAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c8e0add-7302-4a46-fc5d-ffac6d48d654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patched nnx.Variable.set_metadata for QWIX compatibility.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Let's define the configuration we are going to use. Note that this is by no\n",
        "means a \"perfect\" set of hyperparameters. To get good results, you might have\n",
        "to train the model for longer."
      ],
      "metadata": {
        "id": "Eu_NI9nHTn1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Data ======\n",
        "TRAIN_DATA_DIR = \"./data/train\"\n",
        "TEST_DATA_DIR = \"./data/test\"\n",
        "TRAIN_FRACTION = 1.0\n",
        "\n",
        "# ====== LoRA ======\n",
        "RANK = 64\n",
        "ALPHA = 64.0\n",
        "# If you still hit OOM, you can try:\n",
        "# RANK = 32\n",
        "\n",
        "# ====== Sharding ======\n",
        "MESH = [(1, 4), (\"fsdp\", \"tp\")]\n",
        "\n",
        "# ====== GRPO ======\n",
        "# === Generation during GRPO training ===\n",
        "# These control how many tokens are seen per GRPO rollout during training\n",
        "# (not the sampler cache, which we'll handle separately).\n",
        "MAX_PROMPT_LENGTH = 256          # was 256\n",
        "TOTAL_GENERATION_STEPS = 512     # was 512\n",
        "\n",
        "# Important to keep a high-ish temperature for varied, diverse responses during\n",
        "# training.\n",
        "TEMPERATURE = 0.9\n",
        "TOP_P = 1.0\n",
        "TOP_K = 50\n",
        "\n",
        "# The number of times the policy generates multiple responses for a given prompt\n",
        "# within a single training step (G in GRPO).\n",
        "NUM_GENERATIONS = 4              # was 4\n",
        "\n",
        "# === other GRPO configs ===\n",
        "NUM_ITERATIONS = 1\n",
        "BETA = 0.08\n",
        "EPSILON = 0.2\n",
        "\n",
        "# ====== Training ======\n",
        "# Per-device micro-batch size (this is the main memory lever).\n",
        "TRAIN_MICRO_BATCH_SIZE = 4       # was 4\n",
        "\n",
        "# Increase NUM_BATCHES / MAX_STEPS for better results if you have time.\n",
        "NUM_BATCHES = 3738\n",
        "NUM_TEST_BATCHES = 100\n",
        "\n",
        "EVAL_EVERY_N_STEPS = 10\n",
        "NUM_EPOCHS = 1\n",
        "\n",
        "MAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n",
        "\n",
        "# === AdamW, warmup, cosine scheduler ===\n",
        "LEARNING_RATE = 3e-6\n",
        "B1 = 0.9\n",
        "B2 = 0.99\n",
        "WEIGHT_DECAY = 0.1\n",
        "\n",
        "WARMUP_STEPS = 0.1 * MAX_STEPS\n",
        "\n",
        "# == Grad clipping ==\n",
        "MAX_GRAD_NORM = 0.1\n",
        "\n",
        "# Checkpoint saving\n",
        "INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\n",
        "CKPT_DIR = \"/tmp/content/ckpts/\"\n",
        "SAVE_INTERVAL_STEPS = 500\n",
        "MAX_TO_KEEP = 4\n",
        "\n",
        "# ====== Rollout length for GRPO training ======\n",
        "# The sampler cache_size is 704. We choose a safe split:\n",
        "#   256 tokens for prompt + 256 tokens for generation = 512 < 704\n",
        "ROLLOUT_MAX_PROMPT_LENGTH = 256\n",
        "ROLLOUT_GENERATION_STEPS = 256\n",
        "\n",
        "# ====== Evaluation sampling limits ======\n",
        "# For eval (and manual generate()), we keep the same safe envelope.\n",
        "EVAL_MAX_PROMPT_LENGTH = 256\n",
        "EVAL_GENERATION_STEPS = 256\n",
        "\n",
        "# ====== Inference ======\n",
        "GENERATION_CONFIGS = {\n",
        "    \"greedy\": {\"temperature\": 1e-4, \"top_k\": 1, \"top_p\": 1.0},\n",
        "    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
        "    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n",
        "}\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "2cGkRoINwCAJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions"
      ],
      "metadata": {
        "id": "ngjtE-63Tn1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_hbm_usage():\n",
        "  \"\"\"Displays memory usage per device.\"\"\"\n",
        "  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
        "\n",
        "  for d in jax.local_devices():\n",
        "    stats = d.memory_stats()\n",
        "    used = stats[\"bytes_in_use\"]\n",
        "    limit = stats[\"bytes_limit\"]\n",
        "    print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")"
      ],
      "metadata": {
        "id": "wjMFOr7aTn1k",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing\n",
        "\n",
        "First, let's define some special tokens. We instruct the model to first reason\n",
        "between the `<reasoning>` and `</reasoning>` tokens. After\n",
        "reasoning, we expect it to provide the answer between the `<answer>` and\n",
        "`</answer>` tokens."
      ],
      "metadata": {
        "id": "6BtpYMlaTn1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reasoning_start = \"<reasoning>\"\n",
        "reasoning_end = \"</reasoning>\"\n",
        "solution_start = \"<answer>\"\n",
        "solution_end = \"</answer>\"\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT = f\"\"\"You are a Dual-Stream Architecture reasoning model.\n",
        "\n",
        "You must produce two distinct output streams:\n",
        "\n",
        "1. A Monologue Stream between {reasoning_start} and {reasoning_end}\n",
        "   containing your full internal chain-of-thought: planning,\n",
        "   evidence, sanity checks, and any risk scans. Structure it with\n",
        "   markers like [plan], [evidence], [sanity_check],\n",
        "      and [final].\n",
        "\n",
        "2. An Answer Stream between {solution_start} and {solution_end}\n",
        "   containing the final numeric answer.\n",
        "\n",
        "The Monologue Stream and Answer Stream must be coherent with each\n",
        "other and with the question.\n",
        "\n",
        "Always follow exactly this format:\n",
        "\n",
        "{reasoning_start} <reasoning>\n",
        "[plan] ...\n",
        "[evidence] ...\n",
        "[sanity_check] ...\n",
        "[final] ... </reasoning>\n",
        "{reasoning_end}\n",
        "{solution_start} <answer> [Final answer here] </answer> {solution_end}\n",
        "\"\"\"\n",
        "\n",
        "TEMPLATE = \"\"\"<start_of_turn> user:\n",
        "{question}{system_prompt} </turn>\n",
        "<start_of_turn> model:\n",
        "<reasoning>\n",
        "[plan]\n",
        "[evidence] ...\n",
        "[sanity_check] ...\n",
        "[final] ...\n",
        "</reasoning>\n",
        "<answer> ... </answer>\n",
        "</turn>\"\"\"\n"
      ],
      "metadata": {
        "id": "h6RGv1kSTn1k",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use OpenAI's [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k), which comprises grade school math word problems."
      ],
      "metadata": {
        "id": "WASP9N5JTn1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_hash_answer(text: str) -> str | None:\n",
        "  if \"####\" not in text:\n",
        "    return None\n",
        "  return text.split(\"####\")[1].strip()\n",
        "\n",
        "\n",
        "def _load_from_tfds(data_dir: str, split: str):\n",
        "  import tensorflow_datasets.text.gsm8k\n",
        "  return tfds.data_source(\n",
        "      \"gsm8k\",\n",
        "      split=split,\n",
        "      data_dir=data_dir,\n",
        "      builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n",
        "      download=True,\n",
        "  )\n",
        "\n",
        "\n",
        "def download_kaggle_dataset(target_dir=\"./data/gsm8k\"):\n",
        "  os.makedirs(target_dir, exist_ok=True)\n",
        "  src = kagglehub.dataset_download(\"thedevastator/grade-school-math-8k-q-a\")\n",
        "  src = Path(src)\n",
        "  dst = Path(target_dir)\n",
        "\n",
        "  for csv_file in src.glob(\"*.csv\"):  # match all CSV files\n",
        "    shutil.copy2(csv_file, dst / csv_file.name)\n",
        "    print(f\"Copied {csv_file.name} → {dst/csv_file.name}\")\n",
        "  return target_dir\n",
        "\n",
        "\n",
        "def get_dataset(data_dir, split=\"train\", source=\"tfds\") -> grain.MapDataset:\n",
        "  # Download data\n",
        "  if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "  if source == \"tfds\":\n",
        "    import tensorflow_datasets.text.gsm8k\n",
        "    data = tfds.data_source(\n",
        "        \"gsm8k\",\n",
        "        split=split,\n",
        "        data_dir=data_dir,\n",
        "        builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n",
        "        download=True,\n",
        "    )\n",
        "\n",
        "  elif source == \"kaggle\":\n",
        "    kaggle_dir = download_kaggle_dataset(data_dir)\n",
        "    file_name = \"main_\" + split + \".csv\"\n",
        "    csv_path = os.path.join(kaggle_dir, file_name)  # adjust filename if needed\n",
        "\n",
        "    data = []\n",
        "    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "      reader = csv.DictReader(csvfile)\n",
        "      for row in reader:\n",
        "        data.append({\n",
        "            \"question\": row[\"question\"],\n",
        "            \"answer\": row[\"answer\"],\n",
        "        })\n",
        "\n",
        "  elif source == \"huggingface\":\n",
        "    os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
        "    data = load_dataset(\"gsm8k\", \"main\", split=split)\n",
        "\n",
        "  else:\n",
        "    raise ValueError(f\"Unknown source: {source}\")\n",
        "\n",
        "  def _as_text(v):\n",
        "    return v if isinstance(v, str) else v.decode(\"utf-8\")\n",
        "\n",
        "  dataset = (\n",
        "      grain.MapDataset.source(data)\n",
        "      .shuffle(seed=42)\n",
        "      .map(\n",
        "          lambda x: {\n",
        "              # passed to model forward pass\n",
        "              \"prompts\": TEMPLATE.format(\n",
        "                  system_prompt=SYSTEM_PROMPT,\n",
        "                  question=_as_text(x[\"question\"]),\n",
        "              ),\n",
        "              # passed to reward functions\n",
        "              \"question\": _as_text(x[\"question\"]),\n",
        "              # passed to reward functions\n",
        "              \"answer\": extract_hash_answer(_as_text(x[\"answer\"])),\n",
        "          }\n",
        "      )\n",
        "  )\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "gTGjcSMNTn1k",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split the dataset set into train and test sets as usual."
      ],
      "metadata": {
        "id": "uDwobMu_okwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# source = input(\"Choose data source [tfds/kaggle]: \").strip().lower()\n",
        "source = \"huggingface\"\n",
        "\n",
        "if source not in (\"tfds\", \"kaggle\", \"huggingface\"):\n",
        "  print(\"Invalid choice. Defaulting to 'tfds'.\")\n",
        "  source = \"\"\n",
        "\n",
        "print(f\"Using data source: {source}\")\n",
        "\n",
        "dataset = get_dataset(TRAIN_DATA_DIR, \"train\", source).batch(TRAIN_MICRO_BATCH_SIZE)[\n",
        "    :NUM_BATCHES\n",
        "]\n",
        "\n",
        "if TRAIN_FRACTION == 1.0:\n",
        "  train_dataset = dataset.repeat(NUM_EPOCHS)\n",
        "  val_dataset = None\n",
        "else:\n",
        "  train_dataset = dataset[: int(len(dataset) * TRAIN_FRACTION)]\n",
        "  train_dataset = train_dataset.repeat(NUM_EPOCHS)\n",
        "\n",
        "  val_dataset = dataset[int(len(dataset) * TRAIN_FRACTION) :].repeat(NUM_EPOCHS)\n",
        "\n",
        "test_dataset = get_dataset(TEST_DATA_DIR, \"test\", source).batch(TRAIN_MICRO_BATCH_SIZE)[\n",
        "    :NUM_TEST_BATCHES\n",
        "]\n",
        "\n",
        "dataset_lengths = (\n",
        "    len(train_dataset),\n",
        "    len(val_dataset) if val_dataset is not None else 0,\n",
        "    len(test_dataset),\n",
        ")\n",
        "print(f\"dataset contains {dataset_lengths} of batches\")"
      ],
      "metadata": {
        "id": "KXhOL6GyTn1k",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a83510c-0aed-41e3-cb33-72206471cec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using data source: huggingface\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset contains (3738, 0, 100) of batches\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how one batch of the training dataset looks like!\n"
      ],
      "metadata": {
        "id": "k7n8L0VzTn1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ele in train_dataset[:1]:\n",
        "  pprint(ele)"
      ],
      "metadata": {
        "id": "5TF-wNQ2Tn1k",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43617c2c-d0dc-4319-fae7-d63767c766fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': array(['3'], dtype='<U1'),\n",
            " 'prompts': array(['<start_of_turn>user\\nYou are a Dual-Stream Architecture reasoning model.\\n\\nYou must produce two distinct output streams:\\n\\n1. A Monologue Stream between <reasoning> and </reasoning>\\n   containing your full internal chain-of-thought: planning,\\n   evidence, sanity checks, and any risk scans. Structure it with\\n   markers like [plan], [evidence], [sanity_check], [risk_scan],\\n   and [final_internal_action].\\n\\n2. An Answer Stream between <answer> and </answer>\\n   containing only the final numeric answer (no explanation).\\n\\nThe Monologue Stream and Answer Stream must be coherent with each\\nother and with the question.\\n\\nAlways follow exactly this format:\\n\\n<reasoning>\\n[plan] ...\\n[evidence] ...\\n[sanity_check] ...\\n[risk_scan] ...\\n[final_internal_action] ...\\n</reasoning>\\n<answer> <single numeric answer> </answer>\\n\\n\\nMaria has 4 dimes, 4 quarters, and 7 nickels in her piggy bank. Her mom gives her 5 quarters. How much money, in dollars, does Maria have now?<end_of_turn>\\n<start_of_turn>model'],\n",
            "      dtype='<U985'),\n",
            " 'question': array(['Maria has 4 dimes, 4 quarters, and 7 nickels in her piggy bank. Her mom gives her 5 quarters. How much money, in dollars, does Maria have now?'],\n",
            "      dtype='<U142')}\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the policy model and the reference model\n",
        "\n",
        "The policy model is the model which is actually trained and whose weights are\n",
        "updated. The reference model is the model with which we compute KL divergence.\n",
        "This is to ensure that the policy updates are not huge and that it does not\n",
        "deviate too much from the reference model.\n",
        "\n",
        "Typically, the reference model is the base model, and the policy model is the\n",
        "same base model, but with LoRA parameters. Only the LoRA parameters are updated.\n",
        "\n",
        "Note: We perform full precision (fp32) training. You can, however, leverage\n",
        "Qwix for QAT.\n",
        "\n",
        "To load the model, you need to be on [Kaggle](https://www.kaggle.com/) and need\n",
        "to have agreed to the Gemma license\n",
        "[here](https://www.kaggle.com/models/google/gemma/flax/)."
      ],
      "metadata": {
        "id": "BZxBR7Y_Tn1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in\n",
        "if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n",
        "  kagglehub.login()"
      ],
      "metadata": {
        "id": "3GfLHHVYHHKO",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275,
          "referenced_widgets": [
            "00bee22dbb1d47c59dc78bf365ab7a20",
            "878f1ebcd8144381bf64cd2a978d5048",
            "411b782955f64dd78131c39bad1a71e8",
            "0dadc11700fc4fba97d18f64b7b92ac0",
            "ef7219b16cd9417a89e1081f5957a77e",
            "31f9604751994541bc2140ae721c9179",
            "3af59359013346a497ad5df684e879d6",
            "8c7d606d5be84a778611d4704f601b81",
            "20a3f50fd29d4e63add7f69ba7660ef8",
            "b62735f7708741088dbe63713d23ef9f",
            "4e425844e81848209dc88d81b05f6426",
            "687240cba40e405484b985bdd3bb6759",
            "0deb37b87c1747b8b02e33494ad3cbe3",
            "4751ec817941488a94a93bb57387c9dd",
            "5f12640af0624ef7ad1b1848562e0471",
            "39ee94b658ce4ebcbf9fbe9b679501f0",
            "10df9f18d7e1438dbc59b24b0a74b3aa"
          ]
        },
        "outputId": "1f0c0a13-b69d-4608-c2a9-3be2633656ab"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00bee22dbb1d47c59dc78bf365ab7a20"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section would normally describe an intermediate Orbax checkpoint\n",
        "conversion step used in some of the original Tunix GRPO demos.\n",
        "\n",
        "For this **DSAGRPO** version of the notebook we simplify the flow:\n",
        "\n",
        "* We load the Gemma 3 1B-IT weights **directly** into an NNX model using\n",
        "  `tunix.models.gemma3.params`.\n",
        "* We keep that `base_model` in memory and build the LoRA policy and\n",
        "  reinforcement learning stack on top of it.\n",
        "* Orbax is still used internally by Tunix's checkpoint manager during RL\n",
        "  training, but we no longer perform an extra save/restore pass just to\n",
        "  re-materialize the model.\n",
        "\n",
        "This keeps the model-loading path simpler and avoids a class of\n",
        "checkpoint/metadata issues, while remaining fully compatible with the\n",
        "competition requirements.\n"
      ],
      "metadata": {
        "id": "nAghcsT_Pmv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tunix.models.gemma3 import params\n",
        "\n",
        "# Load Gemma3 base model and tokenizer with an intermediate Orbax checkpoint\n",
        "# to keep peak memory usage lower on Kaggle.\n",
        "model_family = \"gemma3\"\n",
        "if model_family != \"gemma3\":\n",
        "  raise ValueError(\"This notebook currently only supports Gemma3-1B-IT.\")\n",
        "\n",
        "MODEL_CP_PATH = params.GEMMA3_1B_IT\n",
        "model_config = model.ModelConfig.gemma3_1b()\n",
        "\n",
        "# Clean out any prior intermediate / training checkpoints.\n",
        "!rm /tmp/content/intermediate_ckpt/* -rf\n",
        "!rm /tmp/content/ckpts/* -rf\n",
        "\n",
        "print(\"Loading Gemma3 1B-IT base model from checkpoint...\")\n",
        "base_model_tmp = params.create_model_from_checkpoint(MODEL_CP_PATH, model_config)\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "_, state = nnx.split(base_model_tmp)\n",
        "checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)\n",
        "checkpointer.wait_until_finished()\n",
        "print(\"Saved intermediate checkpoint to\", INTERMEDIATE_CKPT_DIR)\n",
        "\n",
        "# Delete the temporary in-memory base model to free memory.\n",
        "del base_model_tmp\n",
        "del state\n",
        "gc.collect()\n",
        "\n",
        "# Tokenizer is lightweight, so we can keep it resident in memory.\n",
        "tokenizer = params.create_tokenizer()\n"
      ],
      "metadata": {
        "id": "cIFAxgVOTn1k",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d205d6e-a3c4-4dd9-ddce-841c399dfaa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Gemma3 1B-IT base model from checkpoint...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`StandardCheckpointHandler` expects a target tree to be provided for restore. Not doing so is generally UNSAFE unless you know the present topology to be the same one as the checkpoint was saved under.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved intermediate checkpoint to /tmp/content/intermediate_ckpt/\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "click **Run > Run current and after** on next cell:"
      ],
      "metadata": {
        "id": "SoRUCaY5wCAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Loading and LoRA Application\n",
        "\n",
        "These two functions work together to load a base model from a checkpoint and apply a LoRA (Low-Rank Adaptation) layer to it.\n",
        "\n",
        "* `get_ref_model`: Loads the complete Gemma model from a specified checkpoint path. It uses **JAX sharding** to distribute the model parameters across multiple devices.\n",
        "* `get_lora_model`: Takes the base model and applies LoRA layers to it. It uses a `LoraProvider` to select specific layers (like attention and MLP layers) to be adapted. The resulting LoRA-infused model is then sharded and updated to ensure it's ready for distributed training."
      ],
      "metadata": {
        "id": "hpgXONuORkkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tunix.models.gemma3 import params\n",
        "from tunix.models.gemma3 import model\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import nnx\n",
        "import orbax.checkpoint as ocp\n",
        "import os\n",
        "\n",
        "# Ensure MESH is available (e.g., ((1, 1), ('dp', 'tp')) for single device)\n",
        "\n",
        "def get_gemma_ref_model(ckpt_path: str | None = None):\n",
        "  \"\"\"Return the reference Gemma3 1B model plus mesh and config.\"\"\"\n",
        "  if ckpt_path is None:\n",
        "    ckpt_path = os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n",
        "\n",
        "  # Handle mesh creation. Suppress warning by just calling make_mesh with what we have.\n",
        "  # If MESH is ((1, 1), ('dp', 'tp')), we unpack it.\n",
        "  mesh = jax.make_mesh(*MESH)\n",
        "\n",
        "  model_config = model.ModelConfig.gemma3_1b()\n",
        "\n",
        "  # 1. Build abstract module to get the state structure\n",
        "  # Note: Using model.Gemma3 based on previous context\n",
        "  abs_gemma: nnx.Module = nnx.eval_shape(\n",
        "      lambda: model.Gemma3(model_config, rngs=nnx.Rngs(params=0))\n",
        "  )\n",
        "\n",
        "  abs_state = nnx.state(abs_gemma)\n",
        "  abs_state = jax.tree.map(\n",
        "      lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
        "      abs_state,\n",
        "      nnx.get_named_sharding(abs_state, mesh),\n",
        "  )\n",
        "\n",
        "  # 2. Restore parameters into the abstract state structure\n",
        "  checkpointer = ocp.StandardCheckpointer()\n",
        "  restored_state = checkpointer.restore(ckpt_path, target=abs_state)\n",
        "\n",
        "  # 3. Reconstruct the model using nnx.merge\n",
        "  # 'init_from_state' does not exist in modern nnx.\n",
        "  # We simply merge the graph definition with the restored state.\n",
        "  graph_def, _ = nnx.split(abs_gemma)\n",
        "\n",
        "  # FIX: Use nnx.merge instead of graph_def.init_from_state\n",
        "  base_model = nnx.merge(graph_def, restored_state)\n",
        "\n",
        "  return base_model, mesh, model_config\n",
        "\n",
        "def get_lora_model(base_model, mesh):\n",
        "  \"\"\"Apply LoRA adapters to the base model to obtain the policy model.\"\"\"\n",
        "  lora_provider = qwix.LoraProvider(\n",
        "      module_path=(\n",
        "          \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
        "          \".*attn_vec_einsum\"\n",
        "      ),\n",
        "      rank=RANK,\n",
        "      alpha=ALPHA,\n",
        "  )\n",
        "\n",
        "  model_input = base_model.get_model_input()\n",
        "  lora_model = qwix.apply_lora_to_model(\n",
        "      base_model, lora_provider, **model_input\n",
        "  )\n",
        "\n",
        "  return lora_model"
      ],
      "metadata": {
        "id": "m2KD-nmbTn1k",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we load reference and policy Gemma models using the Flax NNX library and display their structures."
      ],
      "metadata": {
        "id": "mgBALRieR6aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZGKgAaZ2kpv",
        "outputId": "20127002-1b57-48f8-8f84-dafe147743d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Attention',\n",
              " 'AttentionType',\n",
              " 'Block',\n",
              " 'Cache',\n",
              " 'Einsum',\n",
              " 'Embedder',\n",
              " 'FeedForward',\n",
              " 'GEMMA3_ATTENTION_PATTERN',\n",
              " 'Gemma3',\n",
              " 'K_MASK',\n",
              " 'LayerCache',\n",
              " 'ModelConfig',\n",
              " 'QueryPreAttentionNormalisation',\n",
              " 'RMSNorm',\n",
              " 'RematConfig',\n",
              " 'ShardingConfig',\n",
              " 'Tuple',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__spec__',\n",
              " 'apply_rope',\n",
              " 'create_sliding_window_mask',\n",
              " 'dataclasses',\n",
              " 'enum',\n",
              " 'find_last_one_index',\n",
              " 'flax',\n",
              " 'itertools',\n",
              " 'jax',\n",
              " 'jaxtyping',\n",
              " 'jnp',\n",
              " 'nnx',\n",
              " 'pxla',\n",
              " 'shard',\n",
              " 'shd']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference model\n",
        "if model_family == \"gemma3\":\n",
        "  ref_model, mesh, model_config = get_gemma_ref_model()\n"
      ],
      "metadata": {
        "id": "kSdZ7aGhTn1k",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f889250-5b09-493b-efa6-8655ed9892c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1142696015.py:18: DeprecationWarning: The default axis_types will change in JAX v0.9.0 to jax.sharding.AxisType.Explicit. To maintain the old behavior, pass `axis_types=(jax.sharding.AxisType.Auto,) * len(axis_names)`. To opt-into the new behavior, pass `axis_types=(jax.sharding.AxisType.Explicit,) * len(axis_names)\n",
            "  mesh = jax.make_mesh(*MESH)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy model\n",
        "lora_policy = get_lora_model(ref_model, mesh=mesh)\n",
        "# nnx.display(lora_policy)"
      ],
      "metadata": {
        "id": "4i3CfJ1gTn1k",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define reward functions\n",
        "\n",
        "We define four reward functions:\n",
        "\n",
        "- reward if the format of the output exactly matches the instruction given in\n",
        "`TEMPLATE`;\n",
        "- reward if the format of the output approximately matches the instruction given\n",
        "in `TEMPLATE`;\n",
        "- reward if the answer is correct/partially correct;\n",
        "- Sometimes, the text between `<answer>`, `</answer>` might not be one\n",
        "  number. So, we extract the number, and reward the model if the answer is correct.\n",
        "\n",
        "The reward functions are inspired from\n",
        "[here](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb).\n",
        "\n",
        "First off, let's define a RegEx for checking whether the format matches."
      ],
      "metadata": {
        "id": "zLzR1tJfTn1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "match_format = re.compile(\n",
        "    rf\"^[\\s]{{0,}}\"\n",
        "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n",
        "    rf\"{solution_start}(.+?){solution_end}\"\n",
        "    rf\"[\\s]{{0,}}$\",\n",
        "    flags=re.MULTILINE | re.DOTALL,\n",
        ")\n",
        "\n",
        "match_format.search(\n",
        "    f\"{reasoning_start}Let me\"\n",
        "    f\" think!{reasoning_end}{solution_start}2{solution_end}\",\n",
        ")"
      ],
      "metadata": {
        "id": "C7Beft8wTn1k",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd6ed3d-8254-413b-e580-32ee743952af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 54), match='<reasoning>Let me think!</reasoning><answer>2</an>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Give the model a reward of 3 points if the format matches exactly."
      ],
      "metadata": {
        "id": "Fe1rF15zTn1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_format_exactly(prompts, completions, **kwargs):\n",
        "  return [\n",
        "      0 if match_format.search(response) is None else 3.0\n",
        "      for response in completions\n",
        "  ]"
      ],
      "metadata": {
        "id": "_fhQ6pY2Tn1k",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also reward the model if the format of the output matches partially."
      ],
      "metadata": {
        "id": "sWdAdUHuTn1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_format_approximately(prompts, completions, **kwargs):\n",
        "  scores = []\n",
        "\n",
        "  for completion in completions:\n",
        "    score = 0\n",
        "    response = completion\n",
        "    # Count how many keywords are seen - we penalize if too many!\n",
        "    # If we see 1, then plus some points!\n",
        "    score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
        "    score += 0.5 if response.count(reasoning_end) == 1 else -0.5\n",
        "    score += 0.5 if response.count(solution_start) == 1 else -0.5\n",
        "    score += 0.5 if response.count(solution_end) == 1 else -0.5\n",
        "    scores.append(score)\n",
        "  return scores"
      ],
      "metadata": {
        "id": "uOhO4f3-Tn1k",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reward the model if the answer is correct. A reward is also given if the answer\n",
        "does not match exactly, i.e., based on how close the answer is to the correct\n",
        "value."
      ],
      "metadata": {
        "id": "A2fNZDgTTn1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_answer(prompts, completions, answer, **kwargs):\n",
        "  responses = completions\n",
        "\n",
        "  extracted_responses = [\n",
        "      guess.group(1) if (guess := match_format.search(r)) is not None else None\n",
        "      for r in responses\n",
        "  ]\n",
        "\n",
        "  scores = []\n",
        "  assert len(extracted_responses) == len(\n",
        "      answer\n",
        "  ), f\"{extracted_responses} and {answer} have mismatching length\"\n",
        "  for guess, true_answer in zip(extracted_responses, answer):\n",
        "    score = 0\n",
        "    if guess is None:\n",
        "      scores.append(0)\n",
        "      continue\n",
        "    # Correct answer gets 3 points!\n",
        "    if guess == true_answer:\n",
        "      score += 3.0\n",
        "    # Match if spaces are seen\n",
        "    elif guess.strip() == true_answer.strip():\n",
        "      score += 1.5\n",
        "    else:\n",
        "      # We also reward it if the answer is close via ratios!\n",
        "      # Ie if the answer is within some range, reward it!\n",
        "      try:\n",
        "        ratio = float(guess) / float(true_answer)\n",
        "        if ratio >= 0.9 and ratio <= 1.1:\n",
        "          score += 0.5\n",
        "        elif ratio >= 0.8 and ratio <= 1.2:\n",
        "          score += 0.25\n",
        "        else:\n",
        "          score -= 1.0  # Penalize wrong answers\n",
        "      except:\n",
        "        score -= 0.5  # Penalize\n",
        "    scores.append(score)\n",
        "  return scores"
      ],
      "metadata": {
        "id": "S8zcWsmhTn1k",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, the text between `<answer>` and `</answer>` might not be one\n",
        "number; it can be a sentence. So, we extract the number and compare the answer."
      ],
      "metadata": {
        "id": "nIpOVv78Tn1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "match_numbers = re.compile(\n",
        "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\", flags=re.MULTILINE | re.DOTALL\n",
        ")\n",
        "match_numbers.findall(f\"{solution_start}  0.34  {solution_end}\")"
      ],
      "metadata": {
        "id": "NXvRtbk8Tn1k",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e751255a-c8ab-4154-c3f0-cfbe28b34222"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0.34']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def check_numbers(prompts, completions, answer, **kwargs):\n",
        "  question = kwargs[\"question\"]\n",
        "  responses = completions\n",
        "\n",
        "  extracted_responses = [\n",
        "      guess.group(1) if (guess := match_numbers.search(r)) is not None else None\n",
        "      for r in responses\n",
        "  ]\n",
        "\n",
        "  scores = []\n",
        "  print(\"START ============================\")\n",
        "  print(f\"Question: {question[0]}\")\n",
        "  print(f\"Answer: {answer[0]}\")\n",
        "  print(f\"Response: {responses[0]}\")\n",
        "  print(f\"Extracted: {extracted_responses[0]}\")\n",
        "  print(\"END ==============================\")\n",
        "  for guess, true_answer in zip(extracted_responses, answer):\n",
        "    if guess is None:\n",
        "      scores.append(0)\n",
        "      continue\n",
        "    # Convert to numbers\n",
        "    try:\n",
        "      true_answer = float(true_answer.strip())\n",
        "      guess = float(guess.strip())\n",
        "      scores.append(1.5 if guess == true_answer else 0.0)\n",
        "    except:\n",
        "      scores.append(0)\n",
        "      continue\n",
        "  return scores"
      ],
      "metadata": {
        "id": "oxZQAFKOTn1k",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# === Dual-Stream / DSA rewards ===\n",
        "\n",
        "# Heuristic markers that encourage structured internal monologue.\n",
        "section_markers = [\n",
        "    \"[plan]\",\n",
        "    \"[evidence]\",\n",
        "    \"[sanity_check]\",\n",
        "    \"[risk_scan]\",\n",
        "    \"[final_internal_action]\",\n",
        "]\n",
        "\n",
        "\n",
        "def dsa_monologue_structure(prompts, completions, **kwargs):\n",
        "  \"\"\"Reward structured monologues inside <reasoning>...</reasoning>.\"\"\"\n",
        "  scores = []\n",
        "  for completion in completions:\n",
        "    score = 0.0\n",
        "\n",
        "    # Extract monologue block.\n",
        "    m = re.search(\n",
        "        rf\"{re.escape(reasoning_start)}(.*?){re.escape(reasoning_end)}\",\n",
        "        completion,\n",
        "        flags=re.DOTALL | re.MULTILINE,\n",
        "    )\n",
        "    if not m:\n",
        "      scores.append(-1.0)\n",
        "      continue\n",
        "\n",
        "    mono = m.group(1)\n",
        "\n",
        "    # Reward presence of structural markers.\n",
        "    for marker in section_markers:\n",
        "      if marker in mono:\n",
        "        score += 0.25  # up to +1.25 from markers alone\n",
        "\n",
        "    # Reward multi-step reasoning (numbered or bulleted steps).\n",
        "    step_pattern = re.compile(\n",
        "        r\"(^\\s*(\\d+[)\\.:-]|-\\s+))\",\n",
        "        flags=re.MULTILINE,\n",
        "    )\n",
        "    num_steps = len(step_pattern.findall(mono))\n",
        "    if num_steps >= 3:\n",
        "      score += 0.5\n",
        "    elif num_steps >= 1:\n",
        "      score += 0.25\n",
        "    else:\n",
        "      score -= 0.25\n",
        "\n",
        "    # Clamp to a reasonable range.\n",
        "    score = max(-1.0, min(score, 2.0))\n",
        "    scores.append(score)\n",
        "\n",
        "  return scores\n",
        "\n",
        "\n",
        "def dsa_stream_coherence(prompts, completions, answer, **kwargs):\n",
        "  \"\"\"Reward internal coherence between Monologue and Answer streams.\"\"\"\n",
        "  scores = []\n",
        "  for completion in completions:\n",
        "    score = 0.0\n",
        "\n",
        "    # Extract last numeric guess from the monologue.\n",
        "    m_mono = re.search(\n",
        "        rf\"{re.escape(reasoning_start)}(.*?){re.escape(reasoning_end)}\",\n",
        "        completion,\n",
        "        flags=re.DOTALL | re.MULTILINE,\n",
        "    )\n",
        "    monologue_guess = None\n",
        "    if m_mono:\n",
        "      mono = m_mono.group(1)\n",
        "      nums = re.findall(r\"[-+]?\\d*\\.?\\d+\", mono)\n",
        "      if nums:\n",
        "        monologue_guess = nums[-1].strip()\n",
        "\n",
        "    # Extract numeric answer from the <answer> block.\n",
        "    m_ans = match_numbers.search(completion)\n",
        "    answer_guess = m_ans.group(1).strip() if m_ans else None\n",
        "\n",
        "    if monologue_guess is None or answer_guess is None:\n",
        "      scores.append(0.0)\n",
        "      continue\n",
        "\n",
        "    try:\n",
        "      mono_val = float(monologue_guess)\n",
        "      ans_val = float(answer_guess)\n",
        "    except Exception:\n",
        "      scores.append(0.0)\n",
        "      continue\n",
        "\n",
        "    if mono_val == ans_val:\n",
        "      score += 1.0\n",
        "    else:\n",
        "      # If they strongly disagree, penalize.\n",
        "      if ans_val != 0:\n",
        "        ratio = mono_val / ans_val\n",
        "        if 0.9 <= ratio <= 1.1:\n",
        "          score += 0.5  # numerically close\n",
        "        else:\n",
        "          score -= 1.0\n",
        "      else:\n",
        "        score -= 0.5\n",
        "\n",
        "    scores.append(score)\n",
        "\n",
        "  return scores\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "kozbCrQewCAY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# === DSA-GRPO composite reward ===\n",
        "\n",
        "DSA_REWARD_WEIGHTS = {\n",
        "    \"format_exact\": 0.5,\n",
        "    \"format_approx\": 0.25,\n",
        "    \"answer_exact\": 1.5,\n",
        "    \"answer_numeric\": 1.0,\n",
        "    \"monologue_structure\": 0.5,\n",
        "    \"stream_coherence\": 1.0,\n",
        "}\n",
        "\n",
        "\n",
        "def dsa_grpo_reward(prompts, completions, answer, **kwargs):\n",
        "  \"\"\"Composite reward used by DSA-GRPO.\n",
        "\n",
        "  This aggregates:\n",
        "    - format rewards (exact/approx)\n",
        "    - answer correctness (string / numeric)\n",
        "    - monologue structure\n",
        "    - monologue/answer coherence\n",
        "\n",
        "  and gates some of them on basic format + coherence to discourage\n",
        "  'pretty' monologues that disagree with the final answer.\n",
        "  \"\"\"\n",
        "  # Component rewards\n",
        "  r_format_exact = match_format_exactly(prompts, completions, **kwargs)\n",
        "  r_format_approx = match_format_approximately(prompts, completions, **kwargs)\n",
        "  r_answer = check_answer(prompts, completions, answer, **kwargs)\n",
        "  r_numbers = check_numbers(prompts, completions, answer, **kwargs)\n",
        "  r_mono = dsa_monologue_structure(prompts, completions, **kwargs)\n",
        "  r_coh = dsa_stream_coherence(prompts, completions, answer, **kwargs)\n",
        "\n",
        "  rewards = []\n",
        "  for fe, fa, ans, num, mono, coh in zip(\n",
        "      r_format_exact, r_format_approx, r_answer, r_numbers, r_mono, r_coh\n",
        "  ):\n",
        "    total = 0.0\n",
        "\n",
        "    total += DSA_REWARD_WEIGHTS[\"format_exact\"] * fe\n",
        "    total += DSA_REWARD_WEIGHTS[\"format_approx\"] * fa\n",
        "    total += DSA_REWARD_WEIGHTS[\"answer_exact\"] * ans\n",
        "    total += DSA_REWARD_WEIGHTS[\"answer_numeric\"] * num\n",
        "    total += DSA_REWARD_WEIGHTS[\"monologue_structure\"] * mono\n",
        "    total += DSA_REWARD_WEIGHTS[\"stream_coherence\"] * coh\n",
        "\n",
        "    # If format is badly broken, strongly downweight the rest.\n",
        "    if fe <= 0.0 and fa <= 0.0:\n",
        "      total *= 0.25\n",
        "\n",
        "    # If coherence is negative, treat it as a red flag and penalize harder.\n",
        "    if coh < 0.0:\n",
        "      total += coh  # extra penalty\n",
        "      total *= 0.5\n",
        "\n",
        "    rewards.append(float(total))\n",
        "\n",
        "  return rewards\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "kjIk5gm6wCAY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate\n",
        "\n",
        "\n",
        "Before we train the model, let's evaluate the model on the test set so we can\n",
        "see the improvement post training.\n",
        "\n",
        "We evaluate it in two ways:\n",
        "\n",
        "**Quantitative**\n",
        "\n",
        "* **Answer Accuracy**: percentage of samples for which the model predicts the\n",
        "correct final numerical answer  \n",
        "* **Answer (Partial) Accuracy**: percentage of samples for which the model\n",
        "predicts a final numerical answer such that the \\`model answer / answer\\`\n",
        "ratio lies between 0.9 and 1.1.  \n",
        "* **Format Accuracy**: percentage of samples for which the model outputs the\n",
        "correct format, i.e., reasoning between the reasoning special tokens, and the\n",
        "final answer between the \\`\\<start\\_answer\\>\\`, \\`\\<end\\_answer\\>\\` tokens.\n",
        "\n",
        "**Qualitative**\n",
        "\n",
        "We'll also print outputs for a few given questions so that we can compare the generated output later.\n"
      ],
      "metadata": {
        "id": "AaiYMJxFTn1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a helper function to generate an answer, given a prompt."
      ],
      "metadata": {
        "id": "HAaZ7NjBx99P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(\n",
        "    question,\n",
        "    sampler,\n",
        "    temperature: float = 0.7,\n",
        "    top_k: int = 50,\n",
        "    top_p: float = 0.95,\n",
        "    seed: int | None = None,\n",
        "):\n",
        "  \"\"\"Given prompt(s), generates text with cache-safe settings.\"\"\"\n",
        "\n",
        "  # Normalize to a list of questions\n",
        "  if isinstance(question, str):\n",
        "    questions = [question]\n",
        "    single_input = True\n",
        "  else:\n",
        "    questions = list(question)\n",
        "    single_input = False\n",
        "\n",
        "  # Build the actual text prompts using your TEMPLATE / SYSTEM_PROMPT\n",
        "  input_batch = [\n",
        "      TEMPLATE.format(\n",
        "          system_prompt=SYSTEM_PROMPT,\n",
        "          question=q,\n",
        "      )\n",
        "      for q in questions\n",
        "  ]\n",
        "\n",
        "  # ---- Cache-aware length control ----\n",
        "  # Get cache capacity from the sampler\n",
        "  cache_limit = int(sampler.cache_config.cache_size)\n",
        "\n",
        "  # Tokenize prompts to estimate their lengths\n",
        "  # (we use the global `tokenizer` that was used to build the sampler)\n",
        "  tokenized = [tokenizer.encode(s) for s in input_batch]\n",
        "  prompt_lengths = [len(t) for t in tokenized]\n",
        "  max_prompt_len_seen = max(prompt_lengths)\n",
        "\n",
        "  # Desired eval lengths (can be defined in your hyperparam cell, e.g. 256/256)\n",
        "  # If you already have these globals, this will just reuse them.\n",
        "  # Otherwise you can set them here.\n",
        "  try:\n",
        "    desired_prompt_len = EVAL_MAX_PROMPT_LENGTH\n",
        "    desired_gen_steps = EVAL_GENERATION_STEPS\n",
        "  except NameError:\n",
        "    # Fallback if those aren't defined elsewhere\n",
        "    desired_prompt_len = 256\n",
        "    desired_gen_steps = 256\n",
        "\n",
        "  # Never let generation alone exceed half the cache\n",
        "  safe_gen_steps = min(desired_gen_steps, cache_limit // 2)\n",
        "\n",
        "  # Bound the prompt length so that prompt + generation <= cache_limit\n",
        "  # 1. Respect both what we see and the desired cap\n",
        "  tentative_prompt_len = min(max_prompt_len_seen, desired_prompt_len)\n",
        "  # 2. Enforce cache constraint\n",
        "  safe_prompt_len = min(tentative_prompt_len, cache_limit - safe_gen_steps)\n",
        "\n",
        "  # Final safety clamp: if we somehow still exceed cache, shrink generation\n",
        "  if safe_prompt_len + safe_gen_steps > cache_limit:\n",
        "    safe_gen_steps = max(1, cache_limit - safe_prompt_len)\n",
        "\n",
        "  # Debugging prints (optional)\n",
        "  print(\n",
        "      f\"[generate] cache_limit={cache_limit}, \"\n",
        "      f\"max_prompt_len_seen={max_prompt_len_seen}, \"\n",
        "      f\"using max_prompt_length={safe_prompt_len}, \"\n",
        "      f\"max_generation_steps={safe_gen_steps}\"\n",
        "  )\n",
        "\n",
        "  # ---- Call Tunix sampler with safe lengths ----\n",
        "  out_data = sampler(\n",
        "      input_strings=input_batch,\n",
        "      max_prompt_length=int(safe_prompt_len),\n",
        "      max_generation_steps=int(safe_gen_steps),\n",
        "      temperature=temperature,\n",
        "      top_k=top_k,\n",
        "      top_p=top_p,\n",
        "      echo=False,\n",
        "      seed=seed if seed is not None else None,\n",
        "      eos_tokens=[1, 106],\n",
        "  )\n",
        "\n",
        "  output = out_data.text\n",
        "  if single_input:\n",
        "    return output[0]\n",
        "  return output\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "E8oqcpNawCAZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# === Dual-Stream parsing & generation helpers ===\n",
        "\n",
        "def parse_dual_stream(completion: str):\n",
        "  \"\"\"Split a completion into (monologue, answer) strings.\"\"\"\n",
        "  mono_match = re.search(\n",
        "      rf\"{re.escape(reasoning_start)}(.*?){re.escape(reasoning_end)}\",\n",
        "      completion,\n",
        "      flags=re.DOTALL | re.MULTILINE,\n",
        "  )\n",
        "  ans_match = re.search(\n",
        "      rf\"{re.escape(solution_start)}(.*?){re.escape(solution_end)}\",\n",
        "      completion,\n",
        "      flags=re.DOTALL | re.MULTILINE,\n",
        "  )\n",
        "\n",
        "  monologue = mono_match.group(1).strip() if mono_match else completion.strip()\n",
        "  answer = ans_match.group(1).strip() if ans_match else \"\"\n",
        "  return monologue, answer\n",
        "\n",
        "\n",
        "def generate_dual_stream(\n",
        "    question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None\n",
        "):\n",
        "  \"\"\"Wrapper around `generate` that returns (monologue, answer).\"\"\"\n",
        "  raw = generate(\n",
        "      question,\n",
        "      sampler,\n",
        "      temperature=temperature,\n",
        "      top_k=top_k,\n",
        "      top_p=top_p,\n",
        "      seed=seed,\n",
        "  )\n",
        "\n",
        "  if isinstance(raw, str):\n",
        "    return parse_dual_stream(raw)\n",
        "\n",
        "  return [parse_dual_stream(c) for c in raw]\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "aP6F3Au_wCAZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another helper function for evaluation."
      ],
      "metadata": {
        "id": "zNoa5je7yJOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(\n",
        "    dataset,\n",
        "    sampler,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_passes=1,\n",
        "    corr_lst=False,\n",
        "    make_lst=False,\n",
        "):\n",
        "  \"\"\"Computes accuracy and percentage of outputs matching the format.\"\"\"\n",
        "\n",
        "  response_lst = []\n",
        "  corr = 0\n",
        "  partially_corr = 0\n",
        "  corr_format = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch in tqdm(dataset):\n",
        "    answers = batch[\"answer\"]\n",
        "    questions = batch[\"question\"]\n",
        "\n",
        "    multiple_call_responses = [[] for _ in range(len(questions))]\n",
        "    for p in range(num_passes):\n",
        "      responses = generate(\n",
        "          questions, sampler, temperature, top_k, top_p, seed=p\n",
        "      )\n",
        "      for idx, response in enumerate(responses):\n",
        "        multiple_call_responses[idx].append(response)\n",
        "\n",
        "    for question, multiple_call_response, answer in zip(\n",
        "        questions, multiple_call_responses, answers\n",
        "    ):\n",
        "      # check answer\n",
        "      corr_ctr_per_question = 0\n",
        "      partially_corr_per_question = 0\n",
        "      corr_format_per_question = 0\n",
        "      for response in multiple_call_response:\n",
        "        extracted_response = (\n",
        "            guess.group(1)\n",
        "            if (guess := match_numbers.search(response)) is not None\n",
        "            else \"-1000000\"\n",
        "        )\n",
        "        try:\n",
        "          if float(extracted_response.strip()) == float(answer.strip()):\n",
        "            corr_ctr_per_question += 1\n",
        "\n",
        "          ratio = float(extracted_response.strip()) / float(answer.strip())\n",
        "          if ratio >= 0.9 and ratio <= 1.1:\n",
        "            partially_corr_per_question += 1\n",
        "        except:\n",
        "          print(\"SKIPPED\")\n",
        "\n",
        "        # check format\n",
        "        if match_format.search(response) is not None:\n",
        "          corr_format_per_question += 1\n",
        "\n",
        "        if (\n",
        "            corr_ctr_per_question > 0\n",
        "            and partially_corr_per_question > 0\n",
        "            and corr_format_per_question > 0\n",
        "        ):\n",
        "          break\n",
        "\n",
        "      if corr_ctr_per_question > 0:\n",
        "        corr += 1\n",
        "        if corr_lst and make_lst:\n",
        "          response_lst.append((question, answer, multiple_call_response))\n",
        "      else:\n",
        "        if not corr_lst and make_lst:\n",
        "          response_lst.append((question, answer, multiple_call_response))\n",
        "      if partially_corr_per_question > 0:\n",
        "        partially_corr += 1\n",
        "      if corr_format_per_question > 0:\n",
        "        corr_format += 1\n",
        "\n",
        "      total += 1\n",
        "      if total % 10 == 0:\n",
        "        print(\n",
        "            f\"===> {corr=}, {total=}, {corr / total * 100=}, \"\n",
        "            f\"{partially_corr / total * 100=}, {corr_format / total * 100=}\"\n",
        "        )\n",
        "\n",
        "  to_return = (\n",
        "      corr,\n",
        "      total,\n",
        "      corr / total * 100,\n",
        "      partially_corr / total * 100,\n",
        "      corr_format / total * 100,\n",
        "  )\n",
        "  if make_lst:\n",
        "    return to_return, response_lst\n",
        "  return to_return"
      ],
      "metadata": {
        "id": "yJo2nuKB-wlw",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_policy,\n",
        "    tokenizer=tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 512,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "HZMO-KflTn1k",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see how the original model does on the test set. You can see the percentages of the mode outputs that are fully correct, partially correct and just correct in format. The following step might take couple of minutes to finish."
      ],
      "metadata": {
        "id": "UOAQe06DyVlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The evaluation might take up to couple of minutes to finish. Please be patient.\n",
        "\n",
        "(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n",
        "    test_dataset,\n",
        "    sampler,\n",
        "    **GENERATION_CONFIGS[\"greedy\"],\n",
        ")\n",
        "print(\n",
        "    f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n",
        "    f\" {format_accuracy=}%\"\n",
        ")"
      ],
      "metadata": {
        "id": "YQM-tzXWUmoE",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3fec9f48206b44c29c128c9fc056aa1d",
            "412abc012fb142629950545491fd932d",
            "bffe0583f77341e48edff337721e9fc0",
            "30d5c35f19a0489fbc1cb7e22eb954eb",
            "d7feac8adf7f4072b142423c7f74ab20",
            "1ae2408952e54ed9b8b2ae8130947237",
            "21574922368e44f69dcab873e5d5f7ee",
            "22442cc4bdc44225961bf8c70c03a4a0",
            "2d39df45616c420381a6960a69ed4afb",
            "312703de07a54b599c3608c0b5c2c941",
            "9f46c857cb224a2c9e40714901f99e53"
          ]
        },
        "outputId": "3bf5df38-38db-4a68-e424-eafd3f95291b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fec9f48206b44c29c128c9fc056aa1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[generate] cache_limit=960, max_prompt_len_seen=312, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=290, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=332, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=258, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=250, using max_prompt_length=250, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=252, using max_prompt_length=252, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=247, using max_prompt_length=247, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=250, using max_prompt_length=250, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=290, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=256, using max_prompt_length=256, max_generation_steps=256\n",
            "===> corr=0, total=10, corr / total * 100=0.0, partially_corr / total * 100=0.0, corr_format / total * 100=10.0\n",
            "[generate] cache_limit=960, max_prompt_len_seen=280, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=249, using max_prompt_length=249, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=251, using max_prompt_length=251, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=259, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=246, using max_prompt_length=246, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=249, using max_prompt_length=249, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=296, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=271, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=251, using max_prompt_length=251, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=263, using max_prompt_length=256, max_generation_steps=256\n",
            "===> corr=0, total=20, corr / total * 100=0.0, partially_corr / total * 100=0.0, corr_format / total * 100=5.0\n",
            "[generate] cache_limit=960, max_prompt_len_seen=256, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=253, using max_prompt_length=253, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=266, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=257, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=269, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=262, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=251, using max_prompt_length=251, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=259, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=255, using max_prompt_length=255, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=255, using max_prompt_length=255, max_generation_steps=256\n",
            "===> corr=0, total=30, corr / total * 100=0.0, partially_corr / total * 100=0.0, corr_format / total * 100=3.3333333333333335\n",
            "[generate] cache_limit=960, max_prompt_len_seen=247, using max_prompt_length=247, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=299, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=254, using max_prompt_length=254, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=292, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=266, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=279, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=246, using max_prompt_length=246, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=270, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=305, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=288, using max_prompt_length=256, max_generation_steps=256\n",
            "===> corr=0, total=40, corr / total * 100=0.0, partially_corr / total * 100=0.0, corr_format / total * 100=2.5\n",
            "[generate] cache_limit=960, max_prompt_len_seen=240, using max_prompt_length=240, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=327, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=247, using max_prompt_length=247, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=281, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=256, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=265, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=275, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=275, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=262, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=276, using max_prompt_length=256, max_generation_steps=256\n",
            "===> corr=0, total=50, corr / total * 100=0.0, partially_corr / total * 100=0.0, corr_format / total * 100=2.0\n",
            "[generate] cache_limit=960, max_prompt_len_seen=268, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=272, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=298, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=276, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=253, using max_prompt_length=253, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=295, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=258, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=276, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=281, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=325, using max_prompt_length=256, max_generation_steps=256\n",
            "===> corr=0, total=60, corr / total * 100=0.0, partially_corr / total * 100=0.0, corr_format / total * 100=1.6666666666666667\n",
            "[generate] cache_limit=960, max_prompt_len_seen=259, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=266, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=292, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=250, using max_prompt_length=250, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=279, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=287, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=274, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=240, using max_prompt_length=240, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=264, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=247, using max_prompt_length=247, max_generation_steps=256\n",
            "===> corr=0, total=70, corr / total * 100=0.0, partially_corr / total * 100=0.0, corr_format / total * 100=1.4285714285714286\n",
            "[generate] cache_limit=960, max_prompt_len_seen=278, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=288, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=325, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=245, using max_prompt_length=245, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=310, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=321, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=253, using max_prompt_length=253, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=264, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=284, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=279, using max_prompt_length=256, max_generation_steps=256\n",
            "===> corr=0, total=80, corr / total * 100=0.0, partially_corr / total * 100=0.0, corr_format / total * 100=1.25\n",
            "[generate] cache_limit=960, max_prompt_len_seen=293, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=264, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=251, using max_prompt_length=251, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=252, using max_prompt_length=252, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=237, using max_prompt_length=237, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=279, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=251, using max_prompt_length=251, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=317, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=263, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=293, using max_prompt_length=256, max_generation_steps=256\n",
            "SKIPPED\n",
            "===> corr=0, total=90, corr / total * 100=0.0, partially_corr / total * 100=0.0, corr_format / total * 100=1.1111111111111112\n",
            "[generate] cache_limit=960, max_prompt_len_seen=295, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=254, using max_prompt_length=254, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=286, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=274, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=273, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=262, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=246, using max_prompt_length=246, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=284, using max_prompt_length=256, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=244, using max_prompt_length=244, max_generation_steps=256\n",
            "[generate] cache_limit=960, max_prompt_len_seen=333, using max_prompt_length=256, max_generation_steps=256\n",
            "===> corr=0, total=100, corr / total * 100=0.0, partially_corr / total * 100=0.0, corr_format / total * 100=1.0\n",
            "corr=0, total=100, accuracy=0.0%, partial_accuracy=0.0%, format_accuracy=1.0%\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "\n",
        "Let's set up all the configs first - checkpointing, metric logging and training.\n",
        "We then train the model."
      ],
      "metadata": {
        "id": "-CmB2ZT9Tn1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics logger\n",
        "# We rely on Tunix's built-in checkpointing defaults and only configure\n",
        "# metrics logging explicitly here.\n",
        "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
        "    log_dir=\"/tmp/content/tmp/tensorboard/grpo\", flush_every_n_steps=20\n",
        ")"
      ],
      "metadata": {
        "id": "mHzdsYsGTn1l",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer, learning rate scheduler, gradient clipping\n",
        "optimizer = optax.adamw(\n",
        "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
        "        init_value=0.0,\n",
        "        peak_value=LEARNING_RATE,\n",
        "        warmup_steps=WARMUP_STEPS,\n",
        "        decay_steps=MAX_STEPS,\n",
        "        end_value=0.0,\n",
        "    ),\n",
        "    b1=B1,\n",
        "    b2=B2,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        ")\n",
        "if MAX_GRAD_NORM is not None:\n",
        "  optimizer = optax.chain(\n",
        "      optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n",
        "      optimizer,\n",
        "  )"
      ],
      "metadata": {
        "id": "YWvBkWBsruom",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Training config\n",
        "cluster_config = rl_cluster_lib.ClusterConfig(\n",
        "    role_to_mesh={\n",
        "        rl_cluster_lib.Role.ACTOR: mesh,\n",
        "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
        "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
        "    },\n",
        "    rollout_engine='vanilla',\n",
        "    offload_to_cpu=False,\n",
        "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
        "        actor_optimizer=optimizer,\n",
        "        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "        max_steps=MAX_STEPS,\n",
        "        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n",
        "        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n",
        "        # metrics logging\n",
        "        metrics_logging_options=metrics_logging_options,\n",
        "        # checkpoint saving\n",
        "        checkpoint_root_directory=CKPT_DIR,\n",
        "        #checkpointing_options=checkpointing_options,\n",
        "    ),\n",
        "    rollout_config=base_rollout.RolloutConfig(\n",
        "        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
        "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P,\n",
        "        top_k=TOP_K,\n",
        "        eos_tokens=[1,106],\n",
        "    ),\n",
        ")\n",
        "\n",
        "grpo_config = GRPOConfig(\n",
        "    num_generations=NUM_GENERATIONS,\n",
        "    num_iterations=NUM_ITERATIONS,\n",
        "    beta=BETA,\n",
        "    epsilon=EPSILON,\n",
        ")"
      ],
      "metadata": {
        "id": "_6VxFW1ZTn1l",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure RL rollout config uses a safe total sampling length\n",
        "# so we don't exceed sampler.cache_config.cache_size (704).\n",
        "cluster_config.rollout_config.max_prompt_length = ROLLOUT_MAX_PROMPT_LENGTH\n",
        "cluster_config.rollout_config.max_tokens_to_generate = ROLLOUT_GENERATION_STEPS\n",
        "\n",
        "print(\"RolloutConfig.max_prompt_length =\",\n",
        "      cluster_config.rollout_config.max_prompt_length)\n",
        "print(\"RolloutConfig.max_tokens_to_generate =\",\n",
        "      cluster_config.rollout_config.max_tokens_to_generate)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "lD5fYR1twCAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b777fc6-f6dd-42a2-e20c-89b79cee7276"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RolloutConfig.max_prompt_length = 256\n",
            "RolloutConfig.max_tokens_to_generate = 256\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up the GRPO Trainer\n",
        "\n",
        "Now we initialize our system for training. First, we create an `RLCluster` instance, which brings together the **policy model (`actor`)**, a **reference model (`reference`)**, and a **tokenizer**. Our `actor` is a trainable LoRA model, while the `reference` is a fixed base model that we use to guide the training.\n",
        "\n",
        "We then create a `GRPOLearner`, the specialized trainer that uses a list of **reward functions** to evaluate and optimize the model's output, completing the RL training setup.\n",
        "\n",
        "Tunix trainers are integrated with [Weights & Biases](https://wandb.ai/) to help you visualize the training progress. You can choose how you want to use it:\n",
        "\n",
        "**Option 1 (Type 1)**: If you're running a quick experiment or just testing things out, choose this. It creates a temporary, private dashboard right in your browser without requiring you to log in or create an account.\n",
        "\n",
        "**Option 2 (Type 2)**: If you have an existing W&B account and want to save your project's history to your personal dashboard, choose this. You'll be prompted to enter your API key or log in."
      ],
      "metadata": {
        "id": "z4yJWiElSmOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RL cluster\\n\n",
        "rl_cluster = rl_cluster_lib.RLCluster(\n",
        "    actor=lora_policy,\n",
        "    reference=ref_model,\n",
        "    tokenizer=tokenizer,\n",
        "    cluster_config=cluster_config,\n",
        ")\n",
        "\n",
        "# DSA-GRPO Trainer: use composite DSA-aware reward\\n\n",
        "grpo_trainer = GRPOLearner(\n",
        "    rl_cluster=rl_cluster,\n",
        "    reward_fns=[dsa_grpo_reward],\n",
        "    grpo_config=grpo_config,\n",
        ")"
      ],
      "metadata": {
        "id": "OIe1lO08Tn1l",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first couple of training step might take up to 5 minutes to finish. Please be patient. If you experience long training steps, e.g. >10 minutes per step, please open a bug. Really appreciated!"
      ],
      "metadata": {
        "id": "e8b71ed5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with mesh:\n",
        "  grpo_trainer.train(train_dataset)"
      ],
      "metadata": {
        "id": "S27XDebYTn1l",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458,
          "referenced_widgets": [
            "b092aa266c854d85b64c492cd14be50c",
            "9272ecb1985241cabf1d2519e9e10d80",
            "f78bc9682e28481da8430ea756627b62",
            "d7427e66b0ff4b98957556d47a66d900",
            "dc0d01f1926e454ca0ca6862d72833bc",
            "b8dc2d7b5ab0478f9a9c4e94ce688f32",
            "238d92c999214a5b90bb2c49e40d2660",
            "08a67ed427644398b483acd420c1ba6d",
            "35332e2e1ec44419894355c80307ac45",
            "5a8b22ec22b04d2caa746b782ea6429d",
            "6138751932604e27a97f0b8887022ed2"
          ]
        },
        "outputId": "83b7d3a3-7c88-49b8-f4c0-d682f124aa08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "START ============================\n",
            "Question: Maria has 4 dimes, 4 quarters, and 7 nickels in her piggy bank. Her mom gives her 5 quarters. How much money, in dollars, does Maria have now?\n",
            "Answer: 3\n",
            "Response: Please provide the question! I need the question to generate the Monologue and Answer streams as specified.\n",
            "\n",
            "Extracted: None\n",
            "END ==============================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Actor Training:   0%|          | 0/3738 [00:00<?, ?step/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b092aa266c854d85b64c492cd14be50c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Total sampling steps 768 must be less than the cache size 704.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1326844471.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mmesh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mgrpo_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tunix/rl/grpo/grpo_learner.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_ds, eval_ds, skip_jit)\u001b[0m\n\u001b[1;32m    364\u001b[0m       \u001b[0mskip_jit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWhether\u001b[0m \u001b[0mto\u001b[0m \u001b[0mskip\u001b[0m \u001b[0mJIT\u001b[0m \u001b[0mcompilation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \"\"\"\n\u001b[0;32m--> 366\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_jit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tunix/rl/rl_learner.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_ds, eval_ds, skip_jit)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m           \u001b[0;31m# call to throw stop iteration as a singal to break the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m           \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m           \u001b[0;31m# sync the iter steps with internel trainer, this is based on the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m           \u001b[0;31m# assumption that the trainer internally doesn't reset the iter steps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tunix/rl/rl_learner.py\u001b[0m in \u001b[0;36m_prepare_data\u001b[0;34m(self, iterator, proceed_num_steps, sample_repeat, batch_repeat, service_target_batch_size, data_queue, async_loading, mode)\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m       \u001b[0;31m# Signal no more iterable to be loaded.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tunix/rl/rl_learner.py\u001b[0m in \u001b[0;36m_prepare_data\u001b[0;34m(self, iterator, proceed_num_steps, sample_repeat, batch_repeat, service_target_batch_size, data_queue, async_loading, mode)\u001b[0m\n\u001b[1;32m    453\u001b[0m           \u001b[0mproduced_training_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0maccumulated_samples_num\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mservice_target_batch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             produced_training_examples = self._process_accumulated_batches(\n\u001b[0m\u001b[1;32m    456\u001b[0m                 \u001b[0mmicro_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmicro_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 \u001b[0mmicro_batch_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmicro_batch_sizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tunix/rl/rl_learner.py\u001b[0m in \u001b[0;36m_process_accumulated_batches\u001b[0;34m(self, micro_batches, micro_batch_sizes, mode)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0mmerged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrl_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_micro_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmicro_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     \u001b[0mcombined_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_and_compute_advantage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# Split back to original training micro size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tunix/rl/grpo/grpo_learner.py\u001b[0m in \u001b[0;36m_generate_and_compute_advantage\u001b[0;34m(self, training_input, mode)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mpad_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_cluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0meos_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_cluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     rollout_output = self.rl_cluster.generate(\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0mprompts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompts\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tunix/rl/rl_cluster.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, apply_chat_template, mode, micro_batch_size)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0mrollout_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m       outputs = [\n\u001b[0;32m--> 691\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_prompts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrollout_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m           for s in rl_utils.chunk_slices_by_size(\n\u001b[1;32m    693\u001b[0m               \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_prompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmicro_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tunix/rl/rollout/vanilla_rollout.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, rollout_config, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m   ) -> base_rollout.RolloutOutput:\n\u001b[1;32m     54\u001b[0m     \u001b[0;34m\"\"\"Generates samples from the model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     output = self._sampler(\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0minput_strings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mmax_generation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrollout_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_tokens_to_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tunix/generate/sampler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_strings, max_generation_steps, max_prompt_length, echo, return_logits, eos_tokens, forbidden_tokens, temperature, top_p, top_k, beam_size, seed, pad_output)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0mtotal_sampling_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_prompt_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax_generation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_sampling_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m    709\u001b[0m           \u001b[0;34mf'Total sampling steps {total_sampling_steps} must be less than the'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m           \u001b[0;34mf' cache size {self.cache_config.cache_size}.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Total sampling steps 768 must be less than the cache size 704."
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate\n",
        "\n",
        "Let's evaluate our finetuned model!"
      ],
      "metadata": {
        "id": "FzIP8glkTn1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For evaluation we directly use the in-memory `lora_policy`.\n",
        "# After the GRPO training loop above finishes, `lora_policy` already\n",
        "# contains the updated LoRA parameters, so we can construct a sampler\n",
        "# and run evaluation without performing an explicit Orbax restore here.\n",
        "#\n",
        "# If you later want to resume from disk, you could add restore logic\n",
        "# using Tunix's checkpoint manager and Orbax, but that is not required\n",
        "# for the competition workflow.\n"
      ],
      "metadata": {
        "id": "V-73HfP1Tn1l",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_policy,\n",
        "    tokenizer=tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 512,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "1vY9kl-ITn1l",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# The evaluation might take up to couple of minutes to finish. Please be patient.\n",
        "(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n",
        "    test_dataset,\n",
        "    sampler,\n",
        "    **GENERATION_CONFIGS[\"greedy\"],\n",
        ")\n",
        "print(\n",
        "    f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n",
        "    f\" {format_accuracy=}%\"\n",
        ")"
      ],
      "metadata": {
        "id": "nz0q_gGHqYz6",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "With sufficient training, you should see that the percentages of correct model outputs have clearly gone up, which means our training worked."
      ],
      "metadata": {
        "id": "s1NMAxMh0H5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GKd9Tp1d0dZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dual-Stream demo: shows Kaggle-style output fields.\n",
        "demo_question = \"If I have 5 apples and eat 2, how many are left? Answer with a number.\"\n",
        "\n",
        "with mesh:\n",
        "  demo_monologue, demo_answer = generate_dual_stream(\n",
        "      demo_question,\n",
        "      sampler,\n",
        "      **GENERATION_CONFIGS[\"greedy\"],\n",
        "  )\n",
        "\n",
        "print(\"model_thinking_trace:\", demo_monologue)\n",
        "print(\"model_answer:\", demo_answer)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "VR4CxYQJwCAc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the latest trained actor checkpoint for submission/use.\n",
        "# This copies the most recent actor checkpoint directory from CKPT_DIR\n",
        "# into /kaggle/working so it is preserved as a Kaggle artifact.\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "actor_root = os.path.join(CKPT_DIR, \"actor\")\n",
        "if not os.path.isdir(actor_root):\n",
        "  raise ValueError(f\"No actor checkpoints found under {actor_root!r}.\")\n",
        "\n",
        "# Find the latest step_* directory.\n",
        "step_dirs = [\n",
        "    d for d in os.listdir(actor_root)\n",
        "    if d.startswith(\"step_\") and os.path.isdir(os.path.join(actor_root, d))\n",
        "]\n",
        "if not step_dirs:\n",
        "  raise ValueError(f\"No step_* subdirectories found under {actor_root!r}.\")\n",
        "\n",
        "latest_step = max(int(d.replace(\"step_\", \"\")) for d in step_dirs)\n",
        "latest_ckpt_path = os.path.join(actor_root, f\"step_{latest_step}\")\n",
        "print(f\"Latest actor checkpoint: {latest_ckpt_path}\")\n",
        "\n",
        "export_dir = \"/kaggle/working/dsagrpo_gemma3_1b_final\"\n",
        "if os.path.exists(export_dir):\n",
        "  shutil.rmtree(export_dir)\n",
        "\n",
        "shutil.copytree(latest_ckpt_path, export_dir)\n",
        "print(f\"Copied final checkpoint directory to: {export_dir}\")\n",
        "\n",
        "# Optionally, also zip it so there's a single file artifact.\n",
        "zip_base = \"/kaggle/working/dsagrpo_gemma3_1b_final\"\n",
        "shutil.make_archive(zip_base, \"zip\", root_dir=export_dir)\n",
        "print(f\"Zipped final checkpoint to: {zip_base}.zip\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "le_JtlymwCAc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c8922a6"
      },
      "source": [
        "# Task\n",
        "Okay, I will remove the following cells:\n",
        "\n",
        "*   **`y_GHl_szwCAG`**: `os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"`\n",
        "*   **`QaVzUYC_wCAH`**: `from kaggle_secrets import UserSecretsClient`\n",
        "*   **`953CnpN5xQc_`**: `import wandb, os` (which sets `os.environ['WANDB_API_KEY']`)\n",
        "\n",
        "<execute_request>\n",
        "{\"delete_cells\": [\"y_GHl_szwCAG\", \"QaVzUYC_wCAH\", \"953CnpN5xQc_\"]}\n",
        "</execute_request>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b676353"
      },
      "source": [
        "## Remove Environment Variables Cell\n",
        "\n",
        "### Subtask:\n",
        "Identify and delete the code cell that sets environment variables using `os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"` and the cells related to `kaggle_secrets` (`QaVzUYC_wCAH` and `953CnpN5xQc_`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98fef098"
      },
      "source": [
        "Please delete the following code cells from the notebook, as they set environment variables that are either unnecessary or handled differently:\n",
        "\n",
        "*   **Cell ID `y_GHl_szwCAG`**: `import os; os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"`\n",
        "*   **Cell ID `QaVzUYC_wCAH`**: `from kaggle_secrets import UserSecretsClient; ...` (related to `kaggle_secrets`)\n",
        "*   **Cell ID `953CnpN5xQc_`**: `import wandb, os; os.environ['WANDB_API_KEY'] = ...` (related to `kaggle_secrets`)\n",
        "\n",
        "These cells contain redundant or potentially problematic environment variable settings and secret handling for this specific notebook's workflow after the initial setup phase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8b3aa81"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Confirm that the specified environment variable blocks have been removed from the notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87c27ff5"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The specified environment variable blocks have not been removed automatically. Instead, instructions have been provided for manual removal due to system limitations.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The system was unable to programmatically delete the specified code cells.\n",
        "*   Instructions were provided to the user to manually delete three specific cells:\n",
        "    *   **`y_GHl_szwCAG`**: Contained `os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"`.\n",
        "    *   **`QaVzUYC_wCAH`**: Related to `from kaggle_secrets import UserSecretsClient`.\n",
        "    *   **`953CnpN5xQc_`**: Involved `import wandb, os` and setting `os.environ['WANDB_API_KEY']`.\n",
        "*   The cells were marked for removal because they contained \"redundant or potentially problematic environment variable settings and secret handling for this specific notebook's workflow.\"\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Future enhancements should explore direct programmatic cell deletion capabilities to automate such cleanup tasks.\n",
        "*   The user must manually verify the removal of the specified cells to ensure the notebook's environment variable configuration is correct.\n"
      ]
    }
  ]
}